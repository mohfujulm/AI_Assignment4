{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17407c8c",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/mohfujulm/AI_Assignment4/blob/main/Judging%20Flowers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fad10fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "1a5b4d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MJ\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1732: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal.length</th>\n",
       "      <th>sepal.width</th>\n",
       "      <th>petal.length</th>\n",
       "      <th>petal.width</th>\n",
       "      <th>variety</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sepal.length  sepal.width  petal.length  petal.width variety\n",
       "0            5.1          3.5           1.4          0.2       0\n",
       "1            4.9          3.0           1.4          0.2       0\n",
       "2            4.7          3.2           1.3          0.2       0\n",
       "3            4.6          3.1           1.5          0.2       0\n",
       "4            5.0          3.6           1.4          0.2       0\n",
       "5            5.4          3.9           1.7          0.4       0\n",
       "6            4.6          3.4           1.4          0.3       0\n",
       "7            5.0          3.4           1.5          0.2       0\n",
       "8            4.4          2.9           1.4          0.2       0\n",
       "9            4.9          3.1           1.5          0.1       0\n",
       "10           5.4          3.7           1.5          0.2       0\n",
       "11           4.8          3.4           1.6          0.2       0\n",
       "12           4.8          3.0           1.4          0.1       0\n",
       "13           4.3          3.0           1.1          0.1       0\n",
       "14           5.8          4.0           1.2          0.2       0"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset (load remotely, not locally)\n",
    "url = \"https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv\"\n",
    "iris = pd.read_csv(url)\n",
    "\n",
    "#0 = Setosa, 1 = Versicolor, 2 = Virginica\n",
    "iris.variety.loc[iris.variety == 'Setosa'] = 0\n",
    "iris.variety.loc[iris.variety == 'Versicolor'] = 1\n",
    "iris.variety.loc[iris.variety == 'Virginica'] = 2\n",
    "irisNames = [\"Setosa\",\"Versicolor\",\"Virginica\"]\n",
    "\n",
    "# Output the first 15 rows of the data\n",
    "iris.head(15)\n",
    "\n",
    "#warning indicating we are working on a copy, but does not matter for the purposes of this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "fb94a5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal.length  150 non-null    float64\n",
      " 1   sepal.width   150 non-null    float64\n",
      " 2   petal.length  150 non-null    float64\n",
      " 3   petal.width   150 non-null    float64\n",
      " 4   variety       150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "\n",
      "Number of datapoints:  150\n",
      "Average sepal length:  5.843\n",
      "Average sepal width:  3.057\n",
      "Average petal length:  3.758\n",
      "Average petal width:  1.199\n"
     ]
    }
   ],
   "source": [
    "# Display a summary of the table information (number of datapoints, etc.)\n",
    "\n",
    "iris.info()\n",
    "\n",
    "numberDatapoints = iris[\"variety\"].count()\n",
    "averageSepalLength = iris[\"sepal.length\"].mean()\n",
    "averageSepalWidth = iris[\"sepal.width\"].mean()\n",
    "averagePetalLength = iris[\"petal.length\"].mean()\n",
    "averagePetalWidth = iris[\"petal.width\"].mean()\n",
    "\n",
    "print(\"\\nNumber of datapoints: \", round(numberDatapoints, 3))\n",
    "print(\"Average sepal length: \", round(averageSepalLength, 3))\n",
    "print(\"Average sepal width: \", round(averageSepalWidth, 3))\n",
    "print(\"Average petal length: \", round(averagePetalLength, 3))\n",
    "print(\"Average petal width: \", round(averagePetalWidth, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8770949c",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "\n",
    "The iris dataset contains information about some properties of three species of Iris flowers (Iris setosa, Iris virginica, and Iris versicolor).  Namely, these properties are the lengths and widths of both the sepals and petals of the flower.  The final column of the dataset contains the variety of flower corresponding to each datapoint containing these four properties.  \n",
    "\n",
    "As for features and labels, our features (x) are the four given properties of the Iris flowers - sepal and petal length/width.  Our label (y) is the variety of the flower these correspond to, represented by the discrete set [0, 1, 2], with 0 mapping to the setosa flower, 1 mapping to the virginica flower, and 2 mapping to the versicolor flower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0203578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the dataset and split it into our features (X) and label (y)\n",
    "irisX = iris[[\"sepal.length\", \"sepal.width\", \"petal.length\", \"petal.width\"]]\n",
    "irisY = iris[\"variety\"]\n",
    "irisY = irisY.astype('int')\n",
    "\n",
    "# Use sklearn to split the features and labels into a training/test set. (90% train, 10% test)\n",
    "irisX_train, irisX_test, irisY_train, irisY_test = train_test_split(irisX, irisY, train_size = 0.9, test_size=0.1, random_state = 150)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e3da69",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1a516919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample datapoint: \n",
      "    Sepal length:  6.7 , Sepal width:  3.3\n",
      "    Petal length:  5.7 , Petal width:  2.5\n",
      "\n",
      "Actual variety of flower: \n",
      "    Virginica\n",
      "\n",
      "Probabilities of class of flower from model for this datapoint: \n",
      "    Setosa:  0.0 %\n",
      "    Versicolor:  1.37 %\n",
      "    Virginica:  98.63 %\n",
      "\n",
      "Model Score: \n",
      " 0.9333333333333333\n",
      "\n",
      "Coefficients: \n",
      " [[-0.42110328  0.95919094 -2.47288555 -1.06204802]\n",
      " [ 0.58933891 -0.24325485 -0.26514129 -0.86002417]\n",
      " [-0.16823563 -0.71593609  2.73802684  1.92207219]]\n",
      "\n",
      "Intercepts: \n",
      " [  9.68983839   1.73650518 -11.42634357]\n"
     ]
    }
   ],
   "source": [
    "# i. Use sklearn to train a LogisticRegression model on the training set\n",
    "\n",
    "reg = linear_model.LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "reg.fit(irisX_train, irisY_train)\n",
    "\n",
    "# ii. For a sample datapoint, predict the probabilities for each possible class\n",
    "\n",
    "# Sample datapoint from test set \n",
    "sampleTestX = irisX_test.tail(1).values\n",
    "sampleTestY = irisY_test.tail(1).values\n",
    "\n",
    "print(\"Sample datapoint: \")\n",
    "print(\"    Sepal length: \", sampleTestX[0][0], \", Sepal width: \", sampleTestX[0][1])\n",
    "print(\"    Petal length: \", sampleTestX[0][2], \", Petal width: \", sampleTestX[0][3]) \n",
    "print(\"\\nActual variety of flower: \\n   \", irisNames[sampleTestY[0]])     \n",
    "print(\"\\nProbabilities of class of flower from model for this datapoint: \\n    Setosa: \", round(reg.predict_proba(sampleTestX)[0][0]*100,2), \"%\")\n",
    "print(\"    Versicolor: \", round(reg.predict_proba(sampleTestX)[0][1] * 100,2), \"%\")\n",
    "print(\"    Virginica: \", round(reg.predict_proba(sampleTestX)[0][2] * 100,2), \"%\")\n",
    "\n",
    "#print(reg.predict(sampleTestX))\n",
    "\n",
    "# iii. Report on the score for Logistic regression model, what does the score measure?\n",
    "\n",
    "print(\"\\nModel Score: \\n\", reg.score(irisX_test, irisY_test))\n",
    "\n",
    "# iv. Extract the coefficents and intercepts for the boundary line(s)\n",
    "print(\"\\nCoefficients: \\n\", reg.coef_)\n",
    "print(\"\\nIntercepts: \\n\", reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415dd33a",
   "metadata": {},
   "source": [
    "Above we see the score for the Logistic Regression model is about 0.933, a metric calculated using our test values that represents the mean accuracy of our model.  It measures the number of correct predictions made by our model divided by the total number of all predictions, with perfect accuracy being a score of 1.  Thus, a score of 0.933 indicates that the model is overall highly accurate at determining the feature of a particular iris flower given one of its four features using the logistic regression model.\n",
    "\n",
    "The extracted intercepts and coefficients of our equation are listed above.  \n",
    "\n",
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "02261a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample datapoint: \n",
      "    Sepal length:  6.7 , Sepal width:  3.3\n",
      "    Petal length:  5.7 , Petal width:  2.5\n",
      "\n",
      "Actual variety of flower: \n",
      "    Virginica\n",
      "\n",
      "Probabilities of class of flower from model for this datapoint: \n",
      "    Setosa:  0.77 %\n",
      "    Versicolor:  0.14 %\n",
      "    Virginica:  99.1 %\n",
      "\n",
      "Model Score: \n",
      " 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# i. Use sklearn to train a Support Vector Classifier on the training set\n",
    "\n",
    "svmClf = svm.SVC(probability = True)\n",
    "svmClf.fit(irisX_train, irisY_train)\n",
    "\n",
    "# ii. For a sample datapoint, predict the probabilities for each possible class\n",
    "\n",
    "#Utilizing sample datapoint from before\n",
    "print(\"Sample datapoint: \")\n",
    "print(\"    Sepal length: \", sampleTestX[0][0], \", Sepal width: \", sampleTestX[0][1])\n",
    "print(\"    Petal length: \", sampleTestX[0][2], \", Petal width: \", sampleTestX[0][3]) \n",
    "print(\"\\nActual variety of flower: \\n   \", irisNames[sampleTestY[0]])  \n",
    "\n",
    "print(\"\\nProbabilities of class of flower from model for this datapoint: \\n    Setosa: \", round(svmClf.predict_proba(sampleTestX)[0][0]*100,2), \"%\")\n",
    "print(\"    Versicolor: \", round(svmClf.predict_proba(sampleTestX)[0][1] * 100,2), \"%\")\n",
    "print(\"    Virginica: \", round(svmClf.predict_proba(sampleTestX)[0][2] * 100,2), \"%\")\n",
    "\n",
    "#print(svmReg.predict(sampleTestX))\n",
    "\n",
    "# iii. Report on the score for the SVM, what does the score measure?\n",
    "\n",
    "print(\"\\nModel Score: \\n\", svmReg.score(irisX_test, irisY_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286f7676",
   "metadata": {},
   "source": [
    "Above, the score for the SVM model was about 0.933 for this shuffle of the data, once again indicating that the model was highly accurate in predicting the type of Iris flower.  The score is the same as for the Logistic Regression model, meaning that the two models performed about the same in terms of overall accuracy.  For the sample datapoint (using the same for both models), the SVM model seemed to output a marginally higher probability score for the correct flower type, by about 0.4%.\n",
    "\n",
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "75a275f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample datapoint: \n",
      "    Sepal length:  6.7 , Sepal width:  3.3\n",
      "    Petal length:  5.7 , Petal width:  2.5\n",
      "\n",
      "Actual variety of flower: \n",
      "    Virginica\n",
      "\n",
      "Probabilities of class of flower from model for this datapoint: \n",
      "    Setosa:  0.0 %\n",
      "    Versicolor:  0.0 %\n",
      "    Virginica:  100.0 %\n",
      "\n",
      "Model Score Test Data: \n",
      " 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# i. Use sklearn to train a Neural Network (MLP Classifier) on the training set\n",
    "\n",
    "MLPclf = MLPClassifier(activation = 'logistic', solver='lbfgs', max_iter=2000)\n",
    "MLPclf.fit(irisX_train, irisY_train)\n",
    "\n",
    "# ii. For a sample datapoint, predict the probabilities for each possible class\n",
    "\n",
    "#Utilizing sample datapoint from before\n",
    "print(\"Sample datapoint: \")\n",
    "print(\"    Sepal length: \", sampleTestX[0][0], \", Sepal width: \", sampleTestX[0][1])\n",
    "print(\"    Petal length: \", sampleTestX[0][2], \", Petal width: \", sampleTestX[0][3]) \n",
    "print(\"\\nActual variety of flower: \\n   \", irisNames[sampleTestY[0]])  \n",
    "\n",
    "print(\"\\nProbabilities of class of flower from model for this datapoint: \\n    Setosa: \", round(MLPclf.predict_proba(sampleTestX)[0][0]*100,2), \"%\")\n",
    "print(\"    Versicolor: \", round(MLPclf.predict_proba(sampleTestX)[0][1] * 100,2), \"%\")\n",
    "print(\"    Virginica: \", round(MLPclf.predict_proba(sampleTestX)[0][2] * 100,2), \"%\")\n",
    "\n",
    "#print(MLPreg.predict(sampleTestX))\n",
    "\n",
    "# iii. Report on the score for the Neural Network, what does the score measure?\n",
    "\n",
    "print(\"\\nModel Score Test Data: \\n\", MLPclf.score(irisX_test, irisY_test))\n",
    "\n",
    "# iv: Experiment with different options for the neural network, report on your best configuration (the highest score I was able to achieve was 0.8666)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a398c16",
   "metadata": {},
   "source": [
    "The score for the neural network was once again 0.933, identical to the performance of the preceding two models, and indicating that this model is also highly accurate in predicting the type of Iris flower given a datapoint.  Compared to the other two models, the outputted probability of the correct class of flower was at a clear 100% for this shuffle of the data, as opposed to ~ 98.6% - 99.1% in the Logistic Regression/SVM models.\n",
    "\n",
    "The best performance I achieved was using the logistic sigmoid function for the activation function and the lbfgs solver for weight optimization.  With some changes to other parameters, I obtained similar overall accuracy scores, but the probabilities for the 3 classes of flowers were less clear cut for the sample datapoint.  With some parameters such as using the sgd solver and inverse scaling for learning rate, the score declined to values as low as 0.333.  \n",
    "\n",
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0958d8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample datapoint: \n",
      "    Sepal length:  6.7 , Sepal width:  3.3\n",
      "    Petal length:  5.7 , Petal width:  2.5\n",
      "\n",
      "Actual variety of flower: \n",
      "    Virginica\n",
      "\n",
      "Probabilities of class of flower from model for this datapoint: \n",
      "    Setosa:  0.0 %\n",
      "    Versicolor:  0.0 %\n",
      "    Virginica:  100.0 %\n",
      "\n",
      "Model Score: \n",
      " 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# i. Use sklearn to 'train' a k-Neighbors Classifier\n",
    "\n",
    "kNNclf = KNeighborsClassifier(n_neighbors = 10)\n",
    "kNNclf.fit(irisX_train, irisY_train)\n",
    "\n",
    "\n",
    "# Note: KNN is a nonparametric model and technically doesn't require training\n",
    "# fit will essentially load the data into the model see link below for more information\n",
    "# https://stats.stackexchange.com/questions/349842/why-do-we-need-to-fit-a-k-nearest-neighbors-classifier\n",
    "\n",
    "# ii. For a sample datapoint, predict the probabilities for each possible class\n",
    "\n",
    "#Utilizing sample datapoint from before\n",
    "print(\"Sample datapoint: \")\n",
    "print(\"    Sepal length: \", sampleTestX[0][0], \", Sepal width: \", sampleTestX[0][1])\n",
    "print(\"    Petal length: \", sampleTestX[0][2], \", Petal width: \", sampleTestX[0][3]) \n",
    "print(\"\\nActual variety of flower: \\n   \", irisNames[sampleTestY[0]])  \n",
    "\n",
    "print(\"\\nProbabilities of class of flower from model for this datapoint: \\n    Setosa: \", round(kNNclf.predict_proba(sampleTestX)[0][0]*100,2), \"%\")\n",
    "print(\"    Versicolor: \", round(kNNclf.predict_proba(sampleTestX)[0][1] * 100,2), \"%\")\n",
    "print(\"    Virginica: \", round(kNNclf.predict_proba(sampleTestX)[0][2] * 100,2), \"%\")\n",
    "\n",
    "print(\"\\nModel Score: \\n\", MLPclf.score(irisX_test, irisY_test))\n",
    "\n",
    "# iii. Report on the score for kNN, what does the score measure?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af03ff0",
   "metadata": {},
   "source": [
    "Once again, the score for the k-Neighbors Classifier was 0.933, indicating a very high accuracy equivalent to that of the previous models for this random shuffle of the Iris dataset. However, it's probability split for the sample datapoint was equivalent to only that of the MLP classifier/neural net model, with a probability of 100% for the correct flower type for this datapoint.  \n",
    "\n",
    "## Conclusions\n",
    "\n",
    "Overall, all of the models were very good at predicting the type of Iris flower given a datapoint of a flower's sepal length/ width and petal length/width.  They scored highly on their accuracy scores, all of them hitting 0.933 on this particular shuffle of the data.  They vary slightly depending on the particular split of data, but generally score in that range with some slight variation.  \n",
    "\n",
    "The best performing models, given that the scores were all the same, seemed to be the K-Nearest Neighbors and Neural Net/MLP classifier models, as the predicted probability of the correct flower type was usually higher than the other models, and was the highest in this iteration of the notebook, hitting 100% probability for the actual flower type in both models, compared to 98-99% for the other models.  Even so, this difference was very slight.  It is possible that the Neural Network model performed best, compared to models like the logisitic regression because it is capable of producing more complex decision boundaries, which may be needed for this dataset where there is significant overlap between some pairs of the flower classes for certain features.  What surprised me about the exercise was that there was not much variation between the accuracy of the different models given their different approaches to classification of this dataset, and was expecting to see much more clear distinctions in scores/performance of the models.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86dc64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
